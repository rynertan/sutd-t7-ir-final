{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryner\\Music\\sutd-t7-ir-rag\\ragfinal\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.core import Document, Settings, VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from pinecone import Pinecone\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "from typing import List, Optional, Iterator, Dict, Any, Set\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import time\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "import sys\n",
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrieverEvent(Event):\n",
    "    \"\"\"Result of running retrieval\"\"\"\n",
    "\n",
    "    nodes: list[NodeWithScore]\n",
    "\n",
    "\n",
    "class CreateCitationsEvent(Event):\n",
    "    \"\"\"Add citations to the nodes.\"\"\"\n",
    "\n",
    "    nodes: list[NodeWithScore]\n",
    "\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "CITATION_QA_TEMPLATE = PromptTemplate(\n",
    "    \"Please provide an answer based solely on the provided sources. \"\n",
    "    \"When referencing information from a source, \"\n",
    "    \"cite the appropriate source(s) using their corresponding numbers. \"\n",
    "    \"Every answer should include at least one source citation. \"\n",
    "    \"Only cite a source when you are explicitly referencing it. \"\n",
    "    \"If none of the sources are helpful, you should indicate that. \"\n",
    "    # \"For example:\\n\"\n",
    "    # \"Source 1:\\n\"\n",
    "    # \"The sky is red in the evening and blue in the morning.\\n\"\n",
    "    # \"Source 2:\\n\"\n",
    "    # \"Water is wet when the sky is red.\\n\"\n",
    "    # \"Query: When is water wet?\\n\"\n",
    "    # \"Answer: Water will be wet when the sky is red [2], \"\n",
    "    # \"which occurs in the evening [1].\\n\"\n",
    "    # \"Now it's your turn. Below are several numbered sources of information:\"\n",
    "    # \"\\n------\\n\"\n",
    "    # \"{context_str}\"\n",
    "    # \"\\n------\\n\"\n",
    "    # \"Query: {query_str}\\n\"\n",
    "    # \"Answer: \"\n",
    ")\n",
    "\n",
    "CITATION_REFINE_TEMPLATE = PromptTemplate(\n",
    "    \"Please provide an answer based solely on the provided sources. \"\n",
    "    \"When referencing information from a source, \"\n",
    "    \"cite the appropriate source(s) using their corresponding numbers. \"\n",
    "    \"Every answer should include at least one source citation. \"\n",
    "    \"Only cite a source when you are explicitly referencing it. \"\n",
    "    \"If none of the sources are helpful, you should indicate that. \"\n",
    "    # \"For example:\\n\"\n",
    "    # \"Source 1:\\n\"\n",
    "    # \"The sky is red in the evening and blue in the morning.\\n\"\n",
    "    # \"Source 2:\\n\"\n",
    "    # \"Water is wet when the sky is red.\\n\"\n",
    "    # \"Query: When is water wet?\\n\"\n",
    "    # \"Answer: Water will be wet when the sky is red [2], \"\n",
    "    # \"which occurs in the evening [1].\\n\"\n",
    "    # \"Now it's your turn. \"\n",
    "    # \"We have provided an existing answer: {existing_answer}\"\n",
    "    # \"Below are several numbered sources of information. \"\n",
    "    # \"Use them to refine the existing answer. \"\n",
    "    # \"If the provided sources are not helpful, you will repeat the existing answer.\"\n",
    "    # \"\\nBegin refining!\"\n",
    "    # \"\\n------\\n\"\n",
    "    # \"{context_msg}\"\n",
    "    # \"\\n------\\n\"\n",
    "    # \"Query: {query_str}\\n\"\n",
    "    # \"Answer: \"\n",
    ")\n",
    "\n",
    "DEFAULT_CITATION_CHUNK_SIZE = 256\n",
    "DEFAULT_CITATION_CHUNK_OVERLAP = 20\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step,\n",
    ")\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "from llama_index.core.schema import (\n",
    "    MetadataMode,\n",
    "    NodeWithScore,\n",
    "    TextNode,\n",
    ")\n",
    "\n",
    "from llama_index.core.response_synthesizers import (\n",
    "    ResponseMode,\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "\n",
    "from typing import Union, List\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "\n",
    "class CitationQueryEngineWorkflow(Workflow):\n",
    "    @step\n",
    "    async def retrieve(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> Union[RetrieverEvent, None]:\n",
    "        \"Entry point for RAG, triggered by a StartEvent with `query`.\"\n",
    "        query = ev.get(\"query\")\n",
    "        if not query:\n",
    "            return None\n",
    "\n",
    "        print(f\"Query the database with: {query}\")\n",
    "\n",
    "        # store the query in the global context\n",
    "        await ctx.set(\"query\", query)\n",
    "\n",
    "        if ev.index is None:\n",
    "            print(\"Index is empty, load some documents before querying!\")\n",
    "            return None\n",
    "\n",
    "        retriever = ev.index.as_retriever(similarity_top_k=5)\n",
    "        nodes = retriever.retrieve(query)\n",
    "        print(f\"Retrieved {len(nodes)} nodes.\")\n",
    "        return RetrieverEvent(nodes=nodes)\n",
    "\n",
    "    @step\n",
    "    async def create_citation_nodes(\n",
    "        self, ev: RetrieverEvent\n",
    "    ) -> CreateCitationsEvent:\n",
    "        \"\"\"\n",
    "        Modify retrieved nodes to create granular sources for citations.\n",
    "\n",
    "        Takes a list of NodeWithScore objects and splits their content\n",
    "        into smaller chunks, creating new NodeWithScore objects for each chunk.\n",
    "        Each new node is labeled as a numbered source, allowing for more precise\n",
    "        citation in query results.\n",
    "\n",
    "        Args:\n",
    "            nodes (List[NodeWithScore]): A list of NodeWithScore objects to be processed.\n",
    "\n",
    "        Returns:\n",
    "            List[NodeWithScore]: A new list of NodeWithScore objects, where each object\n",
    "            represents a smaller chunk of the original nodes, labeled as a source.\n",
    "        \"\"\"\n",
    "        nodes = ev.nodes\n",
    "\n",
    "        new_nodes: List[NodeWithScore] = []\n",
    "\n",
    "        text_splitter = SentenceSplitter(\n",
    "            chunk_size=DEFAULT_CITATION_CHUNK_SIZE,\n",
    "            chunk_overlap=DEFAULT_CITATION_CHUNK_OVERLAP,\n",
    "        )\n",
    "\n",
    "        for node in nodes:\n",
    "            text_chunks = text_splitter.split_text(\n",
    "                node.node.get_content(metadata_mode=MetadataMode.NONE)\n",
    "            )\n",
    "\n",
    "            for text_chunk in text_chunks:\n",
    "                text = f\"Source {len(new_nodes)+1}:\\n{text_chunk}\\n\"\n",
    "\n",
    "                new_node = NodeWithScore(\n",
    "                    node=TextNode.parse_obj(node.node), score=node.score\n",
    "                )\n",
    "                new_node.node.text = text\n",
    "                new_nodes.append(new_node)\n",
    "        return CreateCitationsEvent(nodes=new_nodes)\n",
    "\n",
    "    @step\n",
    "    async def synthesize(\n",
    "        self, ctx: Context, ev: CreateCitationsEvent\n",
    "    ) -> StopEvent:\n",
    "        \"\"\"Return a streaming response using the retrieved nodes.\"\"\"\n",
    "        llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "        query = await ctx.get(\"query\", default=None)\n",
    "\n",
    "        synthesizer = get_response_synthesizer(\n",
    "            llm=llm,\n",
    "            text_qa_template=CITATION_QA_TEMPLATE,\n",
    "            refine_template=CITATION_REFINE_TEMPLATE,\n",
    "            response_mode=ResponseMode.COMPACT,\n",
    "            use_async=True,\n",
    "        )\n",
    "\n",
    "        response = await synthesizer.asynthesize(query, nodes=ev.nodes)\n",
    "        return StopEvent(result=response)\n",
    "    \n",
    "class PatentIndexBuilder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        embed_model: str = \"text-embedding-3-small\",\n",
    "        openai_api_key: Optional[str] = None,\n",
    "        pinecone_api_key: Optional[str] = None,\n",
    "        pinecone_region: Optional[str] = None,\n",
    "        batch_size: int = 100,\n",
    "        checkpoint_dir: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"Initialize with same parameters as before\"\"\"\n",
    "        load_dotenv(find_dotenv())\n",
    "\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.batch_size = batch_size\n",
    "        self.checkpoint_dir = (\n",
    "            Path(checkpoint_dir) if checkpoint_dir else Path(\"checkpoints\")\n",
    "        )\n",
    "\n",
    "        # API setup\n",
    "        self.openai_api_key = openai_api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "        self.pinecone_api_key = pinecone_api_key or os.getenv(\"PINECONE_API_KEY\")\n",
    "        self.pinecone_region = pinecone_region or os.getenv(\"PINECONE_REGION\")\n",
    "\n",
    "        # Validate\n",
    "        if not self.data_dir.exists():\n",
    "            raise FileNotFoundError(f\"Data directory {data_dir} does not exist\")\n",
    "        if not all([self.openai_api_key, self.pinecone_api_key, self.pinecone_region]):\n",
    "            raise ValueError(\"Missing required API keys or region\")\n",
    "\n",
    "        # Create directories\n",
    "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Initialize Pinecone\n",
    "        self.pc = Pinecone(api_key=self.pinecone_api_key)\n",
    "\n",
    "        # Configure embedding\n",
    "        self.embed_model = OpenAIEmbedding(\n",
    "            model_name=embed_model, api_key=self.openai_api_key, dimensions=1536\n",
    "        )\n",
    "        Settings.embed_model = self.embed_model\n",
    "        Settings.chunk_size = 512\n",
    "        Settings.chunk_overlap = 50\n",
    "\n",
    "        # Logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def get_checkpoint_paths(self) -> tuple[Path, Path]:\n",
    "        \"\"\"Get paths for both processing and embedding checkpoints.\"\"\"\n",
    "        return (\n",
    "            self.checkpoint_dir / \"processing_checkpoint.pkl\",\n",
    "            self.checkpoint_dir / \"embedding_checkpoint.pkl\",\n",
    "        )\n",
    "\n",
    "    def load_checkpoints(self) -> tuple[set, set]:\n",
    "        \"\"\"Load both processing and embedding checkpoints.\"\"\"\n",
    "        proc_path, embed_path = self.get_checkpoint_paths()\n",
    "        processed_ids = set()\n",
    "        embedded_ids = set()\n",
    "\n",
    "        if proc_path.exists():\n",
    "            with open(proc_path, \"rb\") as f:\n",
    "                try:\n",
    "                    checkpoint_data = pickle.load(f)\n",
    "                    processed_ids = checkpoint_data.get(\"document_ids\", set())\n",
    "                    self.logger.info(\n",
    "                        f\"Loaded processing checkpoint: {len(processed_ids)} documents processed\"\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Error loading processing checkpoint: {e}\")\n",
    "                    processed_ids = set()\n",
    "\n",
    "        if embed_path.exists():\n",
    "            with open(embed_path, \"rb\") as f:\n",
    "                try:\n",
    "                    checkpoint_data = pickle.load(f)\n",
    "                    embedded_ids = checkpoint_data.get(\"document_ids\", set())\n",
    "                    self.logger.info(\n",
    "                        f\"Loaded embedding checkpoint: {len(embedded_ids)} documents embedded\"\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Error loading embedding checkpoint: {e}\")\n",
    "                    embedded_ids = set()\n",
    "\n",
    "        return processed_ids, embedded_ids\n",
    "\n",
    "    def save_checkpoint(self, checkpoint_type: str, ids: set):\n",
    "        \"\"\"Save checkpoint for either processing or embedding.\"\"\"\n",
    "        proc_path, embed_path = self.get_checkpoint_paths()\n",
    "        path = proc_path if checkpoint_type == \"processing\" else embed_path\n",
    "\n",
    "        checkpoint_data = {\"document_ids\": ids, \"timestamp\": datetime.now().isoformat()}\n",
    "\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(checkpoint_data, f)\n",
    "        self.logger.info(f\"Saved {checkpoint_type} checkpoint: {len(ids)} documents\")\n",
    "\n",
    "    def process_patent(self, md_file: Path, metadata_dir: Path) -> Optional[Document]:\n",
    "        \"\"\"Process a single patent file and return a Document object.\"\"\"\n",
    "        try:\n",
    "            # Read content\n",
    "            with open(md_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read().strip()\n",
    "\n",
    "            if not content:\n",
    "                self.logger.warning(f\"Empty content in {md_file}\")\n",
    "                return None\n",
    "\n",
    "            # Read metadata\n",
    "            patent_id = md_file.stem\n",
    "            metadata = {\"patent_id\": patent_id}\n",
    "            metadata_file = metadata_dir / f\"{patent_id}.json\"\n",
    "\n",
    "            if metadata_file.exists():\n",
    "                with open(metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    full_metadata = json.load(f)\n",
    "                    metadata.update(\n",
    "                        {\n",
    "                            \"patent_number\": full_metadata.get(\"patent_number\"),\n",
    "                            \"date\": full_metadata.get(\"date\"),\n",
    "                            \"ucid\": full_metadata.get(\"ucid\"),\n",
    "                            \"classification_main\": str(\n",
    "                                full_metadata.get(\"classifications\", {}).get(\"main\")\n",
    "                            ),\n",
    "                            \"classification_further\": str(\n",
    "                                full_metadata.get(\"classifications\", {}).get(\"further\")\n",
    "                            ),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            return Document(text=content, metadata=metadata)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing {md_file}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def stream_documents(self) -> Iterator[tuple[str, Document]]:\n",
    "        \"\"\"Stream documents one at a time, with checkpointing.\"\"\"\n",
    "        content_dir = self.data_dir / \"content\"\n",
    "        metadata_dir = self.data_dir / \"metadata\"\n",
    "\n",
    "        processed_ids, embedded_ids = self.load_checkpoints()\n",
    "\n",
    "        # Get files that haven't been processed or embedded\n",
    "        md_files = [\n",
    "            f\n",
    "            for f in content_dir.glob(\"*.md\")\n",
    "            if f.stem not in processed_ids and f.stem not in embedded_ids\n",
    "        ]\n",
    "\n",
    "        for md_file in tqdm(md_files, desc=\"Processing documents\"):\n",
    "            doc = self.process_patent(md_file, metadata_dir)\n",
    "            if doc:\n",
    "                processed_ids.add(md_file.stem)\n",
    "                if len(processed_ids) % self.batch_size == 0:\n",
    "                    self.save_checkpoint(\"processing\", processed_ids)\n",
    "                yield md_file.stem, doc\n",
    "\n",
    "    @retry(\n",
    "        wait=wait_exponential(multiplier=1, min=4, max=40), stop=stop_after_attempt(3)\n",
    "    )\n",
    "    def embed_batch(self, batch: List[Document], vector_store: PineconeVectorStore):\n",
    "        \"\"\"Embed and index a batch of documents with retry logic and verification.\"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Starting embedding for batch of {len(batch)} documents\")\n",
    "\n",
    "            # Create index from documents\n",
    "            index = VectorStoreIndex.from_documents(\n",
    "                batch, vector_store=vector_store, show_progress=True\n",
    "            )\n",
    "\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error embedding batch: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def build_index(self, index_name: str):\n",
    "        \"\"\"Build and save the patent index using Pinecone with batching.\"\"\"\n",
    "        try:\n",
    "            # Connect to existing index\n",
    "            self.logger.info(f\"Connecting to Pinecone index: {index_name}\")\n",
    "            pinecone_index = self.pc.Index(index_name)\n",
    "\n",
    "            # Get initial stats\n",
    "            initial_stats = pinecone_index.describe_index_stats()\n",
    "            self.logger.info(f\"Initial Pinecone index stats: {initial_stats}\")\n",
    "\n",
    "            # Create vector store and storage context\n",
    "            vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
    "\n",
    "            # Load existing progress\n",
    "            _, embedded_ids = self.load_checkpoints()\n",
    "            self.logger.info(\n",
    "                f\"Resuming from {len(embedded_ids)} previously embedded documents\"\n",
    "            )\n",
    "\n",
    "            # Process and embed in batches\n",
    "            current_batch = []\n",
    "            current_batch_ids = []\n",
    "            total_processed = len(embedded_ids)  # Start from previous progress\n",
    "\n",
    "            self.logger.info(\"Starting document streaming and processing...\")\n",
    "\n",
    "            for patent_id, doc in self.stream_documents():\n",
    "                current_batch.append(doc)\n",
    "                current_batch_ids.append(patent_id)\n",
    "\n",
    "                if len(current_batch) >= self.batch_size:\n",
    "                    self.logger.info(\n",
    "                        f\"Processing batch of {len(current_batch)} documents\"\n",
    "                    )\n",
    "\n",
    "                    # Create index from batch\n",
    "                    VectorStoreIndex.from_documents(\n",
    "                        documents=current_batch,\n",
    "                        storage_context=StorageContext.from_defaults(\n",
    "                            vector_store=vector_store\n",
    "                        ),\n",
    "                    )\n",
    "\n",
    "                    # Update progress\n",
    "                    total_processed += len(current_batch)\n",
    "                    embedded_ids.update(\n",
    "                        current_batch_ids\n",
    "                    )  # Add new IDs to existing set\n",
    "                    self.logger.info(\n",
    "                        f\"Successfully embedded and indexed batch. Total processed: {total_processed}\"\n",
    "                    )\n",
    "\n",
    "                    # Verify in Pinecone\n",
    "                    stats = pinecone_index.describe_index_stats()\n",
    "                    self.logger.info(f\"Pinecone index stats after batch: {stats}\")\n",
    "\n",
    "                    # Save checkpoint with all processed IDs\n",
    "                    self.save_checkpoint(\"embedding\", embedded_ids)\n",
    "\n",
    "                    # Clear batch\n",
    "                    current_batch = []\n",
    "                    current_batch_ids = []\n",
    "\n",
    "                    # Rate limiting\n",
    "                    time.sleep(1)\n",
    "\n",
    "            # Process remaining documents\n",
    "            if current_batch:\n",
    "                self.logger.info(\n",
    "                    f\"Processing final batch of {len(current_batch)} documents\"\n",
    "                )\n",
    "                VectorStoreIndex.from_documents(\n",
    "                    documents=current_batch,\n",
    "                    storage_context=StorageContext.from_defaults(\n",
    "                        vector_store=vector_store\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "                total_processed += len(current_batch)\n",
    "                embedded_ids.update(current_batch_ids)  # Add final batch IDs\n",
    "                self.save_checkpoint(\"embedding\", embedded_ids)\n",
    "                self.logger.info(\n",
    "                    f\"Successfully embedded and indexed final batch. Total processed: {total_processed}\"\n",
    "                )\n",
    "\n",
    "            # Get final stats\n",
    "            final_stats = pinecone_index.describe_index_stats()\n",
    "            self.logger.info(f\"Final Pinecone index stats: {final_stats}\")\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"Completed indexing process. Total documents processed: {total_processed}\"\n",
    "            )\n",
    "            return vector_store\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error building index: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def load_index(\n",
    "        index_name: str, pinecone_api_key: str, embed_model: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"Load existing index (same as before).\"\"\"\n",
    "        try:\n",
    "            pc = Pinecone(api_key=pinecone_api_key)\n",
    "            index = pc.Index(index_name)\n",
    "            vector_store = PineconeVectorStore(pinecone_index=index)\n",
    "\n",
    "            if embed_model:\n",
    "                Settings.embed_model = OpenAIEmbedding(model_name=embed_model)\n",
    "\n",
    "            return VectorStoreIndex.from_vector_store(\n",
    "                vector_store=vector_store, show_progress=True\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading index: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def check_duplicate_vectors(\n",
    "        self, index_name: str, similarity_threshold: float = 0.9999\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Check for duplicate vectors in the Pinecone index by comparing vector similarities.\n",
    "        Only vectors with extremely high similarity (near identical) are considered duplicates.\n",
    "\n",
    "        Args:\n",
    "            index_name: Name of the Pinecone index to check\n",
    "            similarity_threshold: Cosine similarity threshold for considering vectors as duplicates\n",
    "                                (default: 0.9999 for nearly identical vectors)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing duplicate analysis results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Checking for duplicate vectors in index: {index_name}\")\n",
    "\n",
    "            # Connect to the index\n",
    "            index = self.pc.Index(index_name)\n",
    "\n",
    "            # Get index statistics\n",
    "            stats = index.describe_index_stats()\n",
    "            total_vectors = stats.total_vector_count\n",
    "            self.logger.info(f\"Total vectors in index: {total_vectors}\")\n",
    "\n",
    "            # Initialize collections for tracking duplicates\n",
    "            duplicate_groups: List[Dict[str, Any]] = []\n",
    "            processed_ids: Set[str] = set()\n",
    "\n",
    "            # Create a dummy vector for initial query\n",
    "            dummy_vector = [0.0] * 1536  # Dimension of text-embedding-3-small\n",
    "\n",
    "            # Process vectors in batches using query\n",
    "            batch_size = 100\n",
    "            for i in tqdm(range(0, total_vectors, batch_size), desc=\"Checking vectors\"):\n",
    "                # Get a batch of vectors using query\n",
    "                query_response = index.query(\n",
    "                    vector=dummy_vector,\n",
    "                    top_k=batch_size,\n",
    "                    include_values=True,\n",
    "                    include_metadata=True,\n",
    "                )\n",
    "\n",
    "                # Check each vector in the batch\n",
    "                for match in query_response.matches:\n",
    "                    if match.id in processed_ids:\n",
    "                        continue\n",
    "\n",
    "                    # Query for similar vectors using the actual vector\n",
    "                    similar_vectors = index.query(\n",
    "                        vector=match.values,\n",
    "                        top_k=10,  # Limit to most similar matches\n",
    "                        include_metadata=True,\n",
    "                    )\n",
    "\n",
    "                    # Find duplicates above threshold\n",
    "                    duplicates = [\n",
    "                        {\n",
    "                            \"id\": v.id,\n",
    "                            \"score\": v.score,\n",
    "                            \"patent_id\": (\n",
    "                                v.metadata.get(\"patent_id\")\n",
    "                                if hasattr(v, \"metadata\")\n",
    "                                else None\n",
    "                            ),\n",
    "                            \"chunk_index\": (\n",
    "                                v.metadata.get(\"chunk_index\")\n",
    "                                if hasattr(v, \"metadata\")\n",
    "                                else None\n",
    "                            ),\n",
    "                        }\n",
    "                        for v in similar_vectors.matches\n",
    "                        if v.id != match.id and v.score >= similarity_threshold\n",
    "                    ]\n",
    "\n",
    "                    if duplicates:\n",
    "                        group = {\n",
    "                            \"reference_vector\": match.id,\n",
    "                            \"reference_patent\": (\n",
    "                                match.metadata.get(\"patent_id\")\n",
    "                                if hasattr(match, \"metadata\")\n",
    "                                else None\n",
    "                            ),\n",
    "                            \"duplicates\": duplicates,\n",
    "                            \"duplicate_count\": len(duplicates),\n",
    "                        }\n",
    "                        duplicate_groups.append(group)\n",
    "\n",
    "                        # Mark all IDs in this group as processed\n",
    "                        processed_ids.add(match.id)\n",
    "                        processed_ids.update(d[\"id\"] for d in duplicates)\n",
    "\n",
    "            # Create summary\n",
    "            total_duplicates = sum(\n",
    "                group[\"duplicate_count\"] for group in duplicate_groups\n",
    "            )\n",
    "\n",
    "            # Create DataFrame for detailed analysis\n",
    "            if duplicate_groups:\n",
    "                detailed_df = pd.DataFrame(\n",
    "                    [\n",
    "                        {\n",
    "                            \"reference_vector\": group[\"reference_vector\"],\n",
    "                            \"reference_patent\": group[\"reference_patent\"],\n",
    "                            \"duplicate_vector\": dup[\"id\"],\n",
    "                            \"duplicate_patent\": dup[\"patent_id\"],\n",
    "                            \"similarity_score\": dup[\"score\"],\n",
    "                        }\n",
    "                        for group in duplicate_groups\n",
    "                        for dup in group[\"duplicates\"]\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                detailed_df = pd.DataFrame()\n",
    "\n",
    "            summary = {\n",
    "                \"total_vectors\": total_vectors,\n",
    "                \"duplicate_groups\": len(duplicate_groups),\n",
    "                \"total_duplicate_vectors\": total_duplicates,\n",
    "                \"duplicate_percentage\": (\n",
    "                    (total_duplicates / total_vectors * 100) if total_vectors > 0 else 0\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"Found {total_duplicates} duplicate vectors in {len(duplicate_groups)} groups\"\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"summary\": summary,\n",
    "                \"duplicate_groups\": duplicate_groups,\n",
    "                \"detailed_df\": detailed_df,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error checking for duplicate vectors: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def remove_duplicate_vectors(self, index_name: str, dry_run: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Remove duplicate vectors from the index, keeping the first occurrence.\n",
    "\n",
    "        Args:\n",
    "            index_name: Name of the Pinecone index\n",
    "            dry_run: If True, only show what would be deleted without actually deleting\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Starting duplicate vector removal (dry_run={dry_run})\")\n",
    "\n",
    "            # Check for duplicates\n",
    "            duplicate_info = self.check_duplicate_vectors(index_name)\n",
    "\n",
    "            if not duplicate_info[\"duplicate_groups\"]:\n",
    "                self.logger.info(\"No duplicate vectors found. Index is clean.\")\n",
    "                return\n",
    "\n",
    "            # Connect to index\n",
    "            index = self.pc.Index(index_name)\n",
    "\n",
    "            # Collect vectors to delete\n",
    "            vectors_to_delete = []\n",
    "            for group in duplicate_info[\"duplicate_groups\"]:\n",
    "                # Keep the reference vector, delete the duplicates\n",
    "                vectors_to_delete.extend(d[\"id\"] for d in group[\"duplicates\"])\n",
    "\n",
    "            self.logger.info(f\"Found {len(vectors_to_delete)} vectors to delete\")\n",
    "\n",
    "            if dry_run:\n",
    "                self.logger.info(\"Dry run - no vectors will be deleted\")\n",
    "                self.logger.info(f\"Would delete vectors: {vectors_to_delete[:10]}...\")\n",
    "            else:\n",
    "                # Delete in batches\n",
    "                batch_size = 100\n",
    "                for i in range(0, len(vectors_to_delete), batch_size):\n",
    "                    batch = vectors_to_delete[i : i + batch_size]\n",
    "                    index.delete(ids=batch)\n",
    "                    self.logger.info(f\"Deleted batch of {len(batch)} vectors\")\n",
    "\n",
    "                # Verify results\n",
    "                post_cleanup_info = self.check_duplicate_vectors(index_name)\n",
    "                self.logger.info(\n",
    "                    f\"Deduplication complete. Removed {len(vectors_to_delete)} duplicate vectors\"\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error removing duplicate vectors: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['c:\\\\Users\\\\ryner\\\\Music\\\\sutd-t7-ir-rag\\\\ragfinal\\\\Lib\\\\site-packages\\\\pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n",
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['c:\\\\Users\\\\ryner\\\\Music\\\\sutd-t7-ir-rag\\\\ragfinal\\\\Lib\\\\site-packages\\\\pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n"
     ]
    }
   ],
   "source": [
    "builder = PatentIndexBuilder(\n",
    "    data_dir=\"patent_data\",\n",
    "    embed_model=\"text-embedding-3-small\",\n",
    "    batch_size=100,\n",
    "    checkpoint_dir=\"patent_checkpoints\",\n",
    "    pinecone_region=\"us-west-2\",\n",
    ")\n",
    "\n",
    "# Build index\n",
    "index_name = \"patent-search\"\n",
    "\n",
    "# Example query\n",
    "index = PatentIndexBuilder.load_index(\n",
    "    index_name=index_name,\n",
    "    pinecone_api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
    "    embed_model=\"text-embedding-3-small\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = CitationQueryEngineWorkflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: Vacuum cleaners\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5 nodes.\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m w\u001b[38;5;241m.\u001b[39mrun(query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVacuum cleaners\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39mindex)\n",
      "\u001b[1;31mCancelledError\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = await w.run(query=\"Vacuum cleaners\", index=index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation Evaluation - BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Vacuum cleaners are appliances designed for cleaning surfaces by removing dust, dirt, and other particles. They typically consist of a dust chamber for collecting debris and a suction chamber that creates low pressure to draw in air and contaminants. Many vacuum cleaners utilize a cyclone separator to enhance the separation of heavier particles from lighter ones, improving efficiency [1]. Additionally, some models incorporate multiple filter elements to prevent impurities from damaging the motor and to maintain optimal performance [2][4]. The construction of vacuum cleaners often includes molded plastic components for durability and ease of use [5]."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"{result}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vacuum cleaners are appliances designed for cleaning surfaces by removing dust, dirt, and other particles. They typically consist of a dust chamber for collecting debris and a suction chamber that creates low pressure to draw in air and contaminants. Many vacuum cleaners utilize a cyclone separator to enhance the separation of heavier particles from lighter ones, improving efficiency [1]. Additionally, some models incorporate multiple filter elements to prevent impurities from damaging the motor and to maintain optimal performance [2][4]. The construction of vacuum cleaners often includes molded plastic components for durability and ease of use [5].\n"
     ]
    }
   ],
   "source": [
    "print(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Vacuum', 'cleaners', 'are', 'appliances', 'designed', 'for', 'cleaning', 'surfaces', 'by', 'removing', 'dust,', 'dirt,', 'and', 'other', 'particles.', 'They', 'typically', 'consist', 'of', 'a', 'dust', 'chamber', 'for', 'collecting', 'debris', 'and', 'a', 'suction', 'chamber', 'that', 'creates', 'low', 'pressure', 'to', 'draw', 'in', 'air', 'and', 'contaminants.', 'Many', 'vacuum', 'cleaners', 'utilize', 'a', 'cyclone', 'separator', 'to', 'enhance', 'the', 'separation', 'of', 'heavier', 'particles', 'from', 'lighter', 'ones,', 'improving', 'efficiency', '[1].', 'Additionally,', 'some', 'models', 'incorporate', 'multiple', 'filter', 'elements', 'to', 'prevent', 'impurities', 'from', 'damaging', 'the', 'motor', 'and', 'to', 'maintain', 'optimal', 'performance', '[2][4].', 'The', 'construction', 'of', 'vacuum', 'cleaners', 'often', 'includes', 'molded', 'plastic', 'components', 'for', 'durability', 'and', 'ease', 'of', 'use', '[5].']\n"
     ]
    }
   ],
   "source": [
    "candidate = str(result).split()\n",
    "print(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'Vacuum', 'cleaners.', '##', 'Abstract', 'A', 'vacuum', 'cleaner', 'comprising', 'a', 'container', '10', 'for', 'collect', 'ing', 'dust', 'particles', 'and', 'air', 'transport', 'means', '24', 'for', 'drawing', 'air', 'containing', 'the', 'dust', 'particles', 'into', 'the', 'container', '10', 'is', 'addi', 'tionally', 'provided', 'with', 'a', 'frusto', 'conicular', 'cyclone', 'seperator', 'chamber', '16', 'for', 'creating', 'a', 'cyclonic', 'formation', 'of', 'the', 'air', 'caus', 'ing', 'heavier', 'particles', 'to', 'be', 'separated', 'from', 'lighter', 'particles', 'in', 'the', 'air.', '##', 'Claims', 'CLAIMS', '1.', 'A', 'vacuum', 'cleaner', 'for', 'removing', 'particles', 'from', 'a', 'surface', 'comprising', 'a', 'container', 'for', 'collecting', 'at', 'least', 'some', 'of', 'the', 'particles,', 'a', 'frusto', 'conicular', 'chamber', 'adapted', 'to', 'be', 'located', 'relative', 'to', 'the', 'container', 'so', 'that', 'the', 'chamber', 'tapers', 'towards', 'a', 'base', 'of', 'the', 'container,', 'the', 'chamber', 'being', 'open', 'at', 'a', 'lower', 'end', 'portion', 'thereof', 'nearest', 'the', 'base', 'of', 'the', 'container', 'and', 'at', 'an', 'upper', 'end', 'portion', 'thereof', 'and', 'the', 'chamber', 'having', 'an', 'inlet', 'adjacent', 'said', 'upper', 'end', 'portion', 'thereof', 'so', 'that', 'air', 'enters', 'the', 'chamber', 'through', 'the', 'inlet', 'in', 'a', 'tangential', 'direction', 'of', 'the', 'chamber,', 'and', 'air', 'transport', 'means', 'for', 'drawing', 'air', 'from', 'the', 'container', 'in', 'a', 'direction', 'longitudinally', 'of', 'the', 'chamber', 'away', 'from', 'the', 'base', 'of', 'the', 'container,', 'so', 'that', 'the', 'air', 'is', 'first', 'drawn', 'into', 'the', 'chamber', 'through', 'the', 'inlet', 'and', 'flows', 'in', 'a', 'cyclonic', 'path', 'causing', 'heavier', 'particles', 'to', 'be', 'separated', 'from', 'lighter', 'particles', 'in', 'the', 'air.', '2.', 'A', 'vacuum', 'cleaner', 'as', 'claimed', 'in', 'claim', 'I', 'wherein', 'th', 'e', 'chamber', 'is', 'provided', 'with', 'support', 'means', 'whereby', 'the', 'chamber', 'is', 'adapted', 'to', 'be', 'removably', 'located', 'in', 'the', 'container.', '3.', 'A', 'vacuum', 'cleaner', 'as', 'claimed', 'in', 'claim', '2', 'wherein', 'circumferential', 'the', 'support', 'means', 'comprises', 'flange', 'adapted', 'to', 'receive', 'a', 'rim', 'of', 'the', 'container.', '4.', 'A', 'vacuum', 'cleaner', 'as', 'claimed', 'in', 'any', 'one', 'of', 'the', 'is', 'preceding', 'claims', 'wherein', 'the', 'air', 'transport', 'means', 'adapted', 'to', 'be', 'located', 'relative', 'to', 'the', 'container', 'in', 'absence', 'of', 'the', 'chamber.', '5.', 'A', 'vacuum', 'cleaner', 'as', 'claimed', 'in', 'any', 'one', 'of', 'the', 'preceding', 'claims', 'wherein', 'the', 'air', 'transport', 'means', 'comprises', 'a', 'circumferential', 'flange', 'adapted', 'to', 'receive', 'a', 'rim', 'of', 'the', 'container.', '6.', 'A', 'vacuum', 'cleaner', 'as', 'claimed', 'in', 'any', 'one', 'of', 'the', 'preceding', 'claims', 'wherein', 'the', 'chamber', 'is', 'provided', 'with', 'a', 'housing', 'for', 'the', 'air', 'transport', 'means', '7.', 'A', 'vacuum', 'cleaner', 'as', 'claimed', 'in', 'any', 'one', 'of', 'the', 'preceding', 'claims', 'wherein', 'the', 'housing', 'is', 'provided', 'with', 'a', 'filter', 'for', 'collecting', 'at', 'least', 'some', 'of', 'said', 'particles.', '8.']\n",
      "======\n",
      "['2.', 'A', 'vacuum', 'cleaner', 'according', 'to', 'claim', '1,', 'wherein', 'the', 'driving', 'means', 'comprises', 'a', 'cylinder', '18', 'provided', 'on', 'one', 'of', 'the', 'patting', 'vanes', '15', 'and', 'a', 'string', '19', 'slidably', 'passing', 'through', 'the', 'cylinder', '18', ',', 'the', 'string', 'including', 'a', 'grip', 'ball', '20', 'at', 'each', 'end', 'thereof.', '##', 'Description', 'VACUUM', 'CLEANERS', 'This', 'invention', 'relates', 'to', 'vacuum', 'cleaners.', 'Vacuum', 'cleaners', 'are', 'known', 'and', 'widely', 'used', 'for', 'house,', 'factory', 'and', 'office', 'cleaning.', 'Vacuum', 'cleaners', 'are', 'provided', 'with', 'filters', 'which', 'sieve', 'out', 'dust', 'and', 'dirt', 'sucked', 'up', 'by', 'the', 'cleaner.', 'The', 'filter', 'is', 'conventionally', 'in', 'the', 'form', 'of', 'a', 'bag,', 'commonly', 'called', 'a', 'filter', 'bag.', 'However,', 'it', 'is', 'difficult', 'to', 'sieve', 'out', 'the', 'dust', 'and', 'dirt', 'by', 'means', 'of', 'such', 'a', 'filter', 'bag,', 'and', 'unsieved', 'impurities', 'pass', 'through', 'the', 'filter', 'meshes', 'and', 'invade', 'the', 'cage', 'of', 'the', 'motor', 'where', 'they', 'stick', 'to', 'greasy', 'or', 'oily', 'surfaces', 'of', 'the', 'motor', 'components.', 'Eventually,', 'the', 'build', 'up', 'of', 'the', 'impurities', 'causes', 'choking', 'of', 'the', 'motor,', 'and', 'at', 'least', 'reduces', 'its', 'efficiency.', 'Choking', 'of', 'the', 'motor', 'often', 'leads', 'to', 'seizure,', 'and', 'shortens', 'the', 'life', 'of', 'the', 'motor.', 'In', 'addition,', 'the', 'filter', 'elements', 'are', 'also', 'liable', 'to', 'blocking.', 'According', 'to', 'the', 'present', 'invention', 'there', 'is', 'provided', 'a', 'vacuum', 'cleaner', 'characterised', 'by', 'a', 'dust', 'chamber', 'for', 'collecting', 'sucked', 'up', 'dust,', 'dirt', 'and', 'moisture', 'a', 'suction', 'chamber', 'located', 'above', 'the', 'dust', 'chamber,', 'the', 'suction', 'chamber', 'including', 'an', 'air', 'inlet', 'through', 'which,', 'in', 'use,', 'external', 'air', 'is', 'sucked', 'in', 'under', 'low', 'pressure', 'means', 'for', 'causing', 'a', 'low', 'suction', 'pressure', 'in', 'the', 'suction', 'chamber', 'at', 'least', 'one', 'first', 'filter', 'element', 'located', 'between', 'the', 'dust', 'chamber', 'and', 'the', 'suction', 'chamber', 'a', 'plurality', 'of', 'secondary', 'filter', 'elements', 'suspended', 'from', 'a', 'ceiling', 'of', 'the', 'suction', 'chamber,', 'the', 'secondary', 'filter', 'elements', 'being', 'radially', 'arranged', 'around', 'a', 'rotary', 'shaft', 'supported', 'axially', 'of', 'the', 'suction', 'chamber', 'a', 'patting', 'means', 'for', 'causing', 'dust', 'and', 'dirt', 'entrained', 'in', 'the', 'secondary', 'filter', 'elements', 'to', 'fall', 'off', 'by', 'vibration,', 'the', 'patting', 'means', 'comprising', 'a', 'plurality', 'of', 'vanes', 'fixed', 'to', 'the', 'rotary', 'shaft', 'and', 'a', 'single', 'patting', 'arm', 'extending', 'horizontally', 'therefrom', 'and', 'a', 'driving', 'means', 'for', 'rotating', 'the', 'rotary', 'shaft', 'alternately', 'in', 'clockwise', 'and', 'anti', 'clockwise', 'directions.']\n",
      "======\n",
      "['#', 'Vacuum', 'cleaning', 'appliances.', '##', 'Abstract', 'The', 'invention', 'relates', 'to', 'vacuum', 'cleaning', 'appliances.', 'The', 'appliance', 'of', 'the', 'invention', 'includes', 'a', 'cyclone', 'unit', '22', 'which', 'is', 'operable', 'to', 'extract', 'dust', 'and', 'other', 'dirt', 'from', 'the', 'air', 'flow', 'therethrough', 'and', 'to', 'deposit', 'the', 'extracted', 'dust', 'and', 'other', 'dirt', 'in', 'a', 'chamber', '32', 'outside', 'the', 'cyclone', '22', 'and', 'separate', 'from', 'the', 'air', 'flow', 'through', 'the', 'casing', '10', 'of', 'the', 'appliance.', 'The', 'extracted', 'dirt', 'is', 'removed', 'from', 'the', 'ap', 'pliance', 'by', 'separation', 'of', 'the', 'cyclone', 'unit', '22', 'from', 'the', 'cas', 'ing', '10', '.', 'The', 'appliance', 'is', 'convertible', 'to', 'act', 'both', 'as', 'an', 'upright', 'type', 'cleaner', 'of', 'a', 'cylinder', 'type', 'cleaner.']\n",
      "======\n",
      "['#', 'Vacuum', 'cleaners.', '##', 'Abstract', 'A', 'vacuum', 'cleaner', 'includes', 'a', 'dust', 'chamber', '1', ',', 'a', 'suction', 'chamber', '2', 'and', 'means', 'for', 'producing', 'low', 'pressure', 'in', 'the', 'sucking', 'chamber.', 'The', 'dust', 'chamber', '1', 'and', 'the', 'suction', 'chamber', '2', 'are', 'partitioned', 'from', 'one', 'another', 'by', 'a', 'first', 'filter', 'unit', '11', ',', 'and', 'the', 'suction', 'chamber', '2', 'is', 'provided', 'with', 'a', 'secondary', 'filter', 'unit', 'comprising', 'plural', 'filter', 'elements', '14', 'suspended', 'from', 'a', 'ceiling', '13', 'of', 'the', 'suction', 'chamber', '2', 'and', 'arranged', 'radially', 'thereof.', 'A', 'rotary', 'shaft', '16', 'provided', 'at', 'the', 'centre', 'of', 'the', 'radially', 'arranged', 'filter', 'elements', '14', 'supports', 'patting', 'vanes', '15', ',', 'each', 'of', 'which', 'is', 'designed', 'as', 'it', 'rotates', 'to', 'pat', 'each', 'filter', 'element', '14', 'on', 'an', 'inner', 'peripheral', 'portion', 'thereof,', 'thereby', 'causing', 'dust', 'entrained', 'therein', 'to', 'fall', 'off', 'by', 'vibration.', 'The', 'rotary', 'shaft', '16', 'is', 'additionally', 'provided', 'with', 'a', 'single', 'patting', 'arm', '17', 'extending', 'horizontally', 'so', 'as', 'to', 'pat', 'each', 'filter', 'element', '14', 'on', 'a', 'bottom', 'portion', 'thereof.', '##', 'Claims', 'CLAIMS', '1.', 'A', 'vacuum', 'cleaner', 'characterised', 'by', 'a', 'dust', 'chamber', '1', 'for', 'collecting', 'sucked', 'up', 'dust,', 'dirt', 'and', 'moisture', 'a', 'suction', 'chamber', '2', 'located', 'above', 'the', 'dust', 'chamber', '1', ',', 'the', 'suction', 'chamber', '2', 'including', 'an', 'air', 'inlet', '3', 'through', 'which,', 'in', 'use,', 'external', 'air', 'is', 'sucked', 'in', 'under', 'low', 'pressure', 'means', 'for', 'causing', 'a', 'low', 'suction', 'pressure', 'in', 'the', 'suction', 'chamber', '2', 'at', 'least', 'one', 'first', 'filter', 'element', '11', 'located', 'between', 'the', 'dust', 'chamber', '1', 'and', 'the', 'suction', 'chamber', '2', 'a', 'plurality', 'of', 'secondary', 'filter', 'elements', '14', 'suspended', 'from', 'a', 'ceiling', '13', 'of', 'the', 'suction', 'chamber', '2', ',', 'the', 'secondary', 'filter', 'elements', '14', 'being', 'radially', 'arranged', 'around', 'a', 'rotary', 'shaft', '16', 'supported', 'axially', 'of', 'the', 'suction', 'chamber', '2', 'a', 'patting', 'means', 'for', 'causing', 'dust', 'and', 'dirt', 'entrained', 'in', 'the', 'secondary', 'filter', 'elements', '14', 'to', 'fall', 'off', 'by', 'vibration,', 'the', 'patting', 'means', 'comprising', 'a', 'plurality', 'of', 'vanes', '15', 'fixed', 'to', 'the', 'rotary', 'shaft', '10', 'and', 'a', 'single', 'patting', 'arm', '13', 'extending', 'horizontally', 'therefrom', 'and', 'a', 'driving', 'means', 'for', 'rotating', 'the', 'rotary', 'shaft', '16', 'alternately', 'in', 'clockwise', 'and', 'anti', 'clockwise', 'directions.', '2.']\n",
      "======\n",
      "['#', 'Compact', 'vacuum', 'cleaner.', '##', 'Abstract', 'A', 'vacuum', 'cleaner', '10', 'is', 'constructed', 'of', 'relatively', 'rigid,', 'molded', 'plastic', 'main', 'structural', 'elements', 'including', 'a', 'dirt', 'col', 'lecting', 'tank', '12', 'and', 'a', 'motor', 'housing', '14', 'releaseably', 'secured', 'to', 'each', 'other', 'by', 'a', 'buckle', '35.', 'The', 'main', 'structural', 'plastic', 'ele', 'ments', 'also', 'include', 'a', 'fan', 'housing', '30', 'wherein', 'a', 'centrifugal', 'fan', '51', 'rotates,', 'a', 'mounting', 'plate', '55', 'to', 'which', 'the', 'motor', '25', 'is', 'secured,', 'and', 'a', 'baffle', 'member', '60', 'having', 'a', 'cup', 'formation', 'which', 'receives', 'the', 'rear', 'of', 'the', 'motor', '25.', 'A', 'common', 'fasten', 'ing', 'means', '121', 'mechanically', 'secures', 'the', 'fan', 'housing', '30,', 'the', 'plate', '55,', 'and', 'the', 'baffle', 'member', '60', 'to', 'the', 'motor', 'housing', '14.', 'A', 'releaseable', 'latch', '34', 'is', 'at', 'one', 'end', 'of', 'the', 'buckle', '35', 'and', 'a', 'hook', '33', 'at', 'the', 'other', 'end', 'thereof.', 'The', 'latch', '34', 'holds', 'the', 'tank', '12', 'and', 'motor', 'housing', '14', 'together,', 'and', 'the', 'hook', '33', 'is', 'for', 'en', 'gaging', 'a', 'wall', 'bracket', '110', 'to', 'mount', 'the', 'cleaner', '10', 'in', 'a', 'ver', 'tical', 'position.', 'The', 'buckle', '35', 'also', 'includes', 'a', 'skid', 'portion', '32,', 'located', 'between', 'the', 'latch', '34', 'and', 'hook', '33,', 'to', 'facilitate', 'move', 'ment', 'of', 'the', 'cleaner', '10', 'along', 'a', 'horizontal', 'supporting', 'surface', '31.', 'Hundreds', 'of', 'relatively', 'small', 'apertures', '95', 'in', 'the', 'mount', 'ing', 'plate', '55', 'are', 'arranged', 'in', 'a', 'narrow', 'band', 'adja', 'cent', 'the', 'periphery', 'of', 'the', 'fan', '51', 'so', 'as', 'to', 'provide', 'for', 'the', 'pas', 'sage', 'of', 'air', 'through', 'the', 'motor', 'mounting', 'plate', '55', 'without', 'creating', 'excessive', 'back', 'pressure', 'while', 'maintaining', 'quiet', 'conditions', 'in', 'spite', 'of', 'high', 'speed', 'air', 'flow.']\n",
      "======\n",
      "[['#', 'Vacuum', 'cleaners.', '##', 'Abstract', 'A', 'vacuum', 'cleaner', 'comprising', 'a', 'container', '10', 'for', 'collect', 'ing', 'dust', 'particles', 'and', 'air', 'transport', 'means', '24', 'for', 'drawing', 'air', 'containing', 'the', 'dust', 'particles', 'into', 'the', 'container', '10', 'is', 'addi', 'tionally', 'provided', 'with', 'a', 'frusto', 'conicular', 'cyclone', 'seperator', 'chamber', '16', 'for', 'creating', 'a', 'cyclonic', 'formation', 'of', 'the', 'air', 'caus', 'ing', 'heavier', 'particles', 'to', 'be', 'separated', 'from', 'lighter', 'particles', 'in', 'the', 'air.', '##', 'Claims', 'CLAIMS', '1.', 'A', 'vacuum', 'cleaner', 'for', 'removing', 'particles', 'from', 'a', 'surface', 'comprising', 'a', 'container', 'for', 'collecting', 'at', 'least', 'some', 'of', 'the', 'particles,', 'a', 'frusto', 'conicular', 'chamber', 'adapted', 'to', 'be', 'located', 'relative', 'to', 'the', 'container', 'so', 'that', 'the', 'chamber', 'tapers', 'towards', 'a', 'base', 'of', 'the', 'container,', 'the', 'chamber', 'being', 'open', 'at', 'a', 'lower', 'end', 'portion', 'thereof', 'nearest', 'the', 'base', 'of', 'the', 'container', 'and', 'at', 'an', 'upper', 'end', 'portion', 'thereof', 'and', 'the', 'chamber', 'having', 'an', 'inlet', 'adjacent', 'said', 'upper', 'end', 'portion', 'thereof', 'so', 'that', 'air', 'enters', 'the', 'chamber', 'through', 'the', 'inlet', 'in', 'a', 'tangential', 'direction', 'of', 'the', 'chamber,', 'and', 'air', 'transport', 'means', 'for', 'drawing', 'air', 'from', 'the', 'container', 'in', 'a', 'direction', 'longitudinally', 'of', 'the', 'chamber', 'away', 'from', 'the', 'base', 'of', 'the', 'container,', 'so', 'that', 'the', 'air', 'is', 'first', 'drawn', 'into', 'the', 'chamber', 'through', 'the', 'inlet', 'and', 'flows', 'in', 'a', 'cyclonic', 'path', 'causing', 'heavier', 'particles', 'to', 'be', 'separated', 'from', 'lighter', 'particles', 'in', 'the', 'air.', '2.', 'A', 'vacuum', 'cleaner', 'as', 'claimed', 'in', 'claim', 'I', 'wherein', 'th', 'e', 'chamber', 'is', 'provided', 'with', 'support', 'means', 'whereby', 'the', 'chamber', 'is', 'adapted', 'to', 'be', 'removably', 'located', 'in', 'the', 'container.', '3.', 'A', 'vacuum', 'cleaner', 'as', 'claimed', 'in', 'claim', '2', 'wherein', 'circumferential', 'the', 'support', 'means', 'comprises', 'flange', 'adapted', 'to', 'receive', 'a', 'rim', 'of', 'the', 'container.', '4.', 'A', 'vacuum', 'cleaner', 'as', 'claimed', 'in', 'any', 'one', 'of', 'the', 'is', 'preceding', 'claims', 'wherein', 'the', 'air', 'transport', 'means', 'adapted', 'to', 'be', 'located', 'relative', 'to', 'the', 'container', 'in', 'absence', 'of', 'the', 'chamber.', '5.', 'A', 'vacuum', 'cleaner', 'as', 'claimed', 'in', 'any', 'one', 'of', 'the', 'preceding', 'claims', 'wherein', 'the', 'air', 'transport', 'means', 'comprises', 'a', 'circumferential', 'flange', 'adapted', 'to', 'receive', 'a', 'rim', 'of', 'the', 'container.', '6.', 'A', 'vacuum', 'cleaner', 'as', 'claimed', 'in', 'any', 'one', 'of', 'the', 'preceding', 'claims', 'wherein', 'the', 'chamber', 'is', 'provided', 'with', 'a', 'housing', 'for', 'the', 'air', 'transport', 'means', '7.', 'A', 'vacuum', 'cleaner', 'as', 'claimed', 'in', 'any', 'one', 'of', 'the', 'preceding', 'claims', 'wherein', 'the', 'housing', 'is', 'provided', 'with', 'a', 'filter', 'for', 'collecting', 'at', 'least', 'some', 'of', 'said', 'particles.', '8.'], ['2.', 'A', 'vacuum', 'cleaner', 'according', 'to', 'claim', '1,', 'wherein', 'the', 'driving', 'means', 'comprises', 'a', 'cylinder', '18', 'provided', 'on', 'one', 'of', 'the', 'patting', 'vanes', '15', 'and', 'a', 'string', '19', 'slidably', 'passing', 'through', 'the', 'cylinder', '18', ',', 'the', 'string', 'including', 'a', 'grip', 'ball', '20', 'at', 'each', 'end', 'thereof.', '##', 'Description', 'VACUUM', 'CLEANERS', 'This', 'invention', 'relates', 'to', 'vacuum', 'cleaners.', 'Vacuum', 'cleaners', 'are', 'known', 'and', 'widely', 'used', 'for', 'house,', 'factory', 'and', 'office', 'cleaning.', 'Vacuum', 'cleaners', 'are', 'provided', 'with', 'filters', 'which', 'sieve', 'out', 'dust', 'and', 'dirt', 'sucked', 'up', 'by', 'the', 'cleaner.', 'The', 'filter', 'is', 'conventionally', 'in', 'the', 'form', 'of', 'a', 'bag,', 'commonly', 'called', 'a', 'filter', 'bag.', 'However,', 'it', 'is', 'difficult', 'to', 'sieve', 'out', 'the', 'dust', 'and', 'dirt', 'by', 'means', 'of', 'such', 'a', 'filter', 'bag,', 'and', 'unsieved', 'impurities', 'pass', 'through', 'the', 'filter', 'meshes', 'and', 'invade', 'the', 'cage', 'of', 'the', 'motor', 'where', 'they', 'stick', 'to', 'greasy', 'or', 'oily', 'surfaces', 'of', 'the', 'motor', 'components.', 'Eventually,', 'the', 'build', 'up', 'of', 'the', 'impurities', 'causes', 'choking', 'of', 'the', 'motor,', 'and', 'at', 'least', 'reduces', 'its', 'efficiency.', 'Choking', 'of', 'the', 'motor', 'often', 'leads', 'to', 'seizure,', 'and', 'shortens', 'the', 'life', 'of', 'the', 'motor.', 'In', 'addition,', 'the', 'filter', 'elements', 'are', 'also', 'liable', 'to', 'blocking.', 'According', 'to', 'the', 'present', 'invention', 'there', 'is', 'provided', 'a', 'vacuum', 'cleaner', 'characterised', 'by', 'a', 'dust', 'chamber', 'for', 'collecting', 'sucked', 'up', 'dust,', 'dirt', 'and', 'moisture', 'a', 'suction', 'chamber', 'located', 'above', 'the', 'dust', 'chamber,', 'the', 'suction', 'chamber', 'including', 'an', 'air', 'inlet', 'through', 'which,', 'in', 'use,', 'external', 'air', 'is', 'sucked', 'in', 'under', 'low', 'pressure', 'means', 'for', 'causing', 'a', 'low', 'suction', 'pressure', 'in', 'the', 'suction', 'chamber', 'at', 'least', 'one', 'first', 'filter', 'element', 'located', 'between', 'the', 'dust', 'chamber', 'and', 'the', 'suction', 'chamber', 'a', 'plurality', 'of', 'secondary', 'filter', 'elements', 'suspended', 'from', 'a', 'ceiling', 'of', 'the', 'suction', 'chamber,', 'the', 'secondary', 'filter', 'elements', 'being', 'radially', 'arranged', 'around', 'a', 'rotary', 'shaft', 'supported', 'axially', 'of', 'the', 'suction', 'chamber', 'a', 'patting', 'means', 'for', 'causing', 'dust', 'and', 'dirt', 'entrained', 'in', 'the', 'secondary', 'filter', 'elements', 'to', 'fall', 'off', 'by', 'vibration,', 'the', 'patting', 'means', 'comprising', 'a', 'plurality', 'of', 'vanes', 'fixed', 'to', 'the', 'rotary', 'shaft', 'and', 'a', 'single', 'patting', 'arm', 'extending', 'horizontally', 'therefrom', 'and', 'a', 'driving', 'means', 'for', 'rotating', 'the', 'rotary', 'shaft', 'alternately', 'in', 'clockwise', 'and', 'anti', 'clockwise', 'directions.'], ['#', 'Vacuum', 'cleaning', 'appliances.', '##', 'Abstract', 'The', 'invention', 'relates', 'to', 'vacuum', 'cleaning', 'appliances.', 'The', 'appliance', 'of', 'the', 'invention', 'includes', 'a', 'cyclone', 'unit', '22', 'which', 'is', 'operable', 'to', 'extract', 'dust', 'and', 'other', 'dirt', 'from', 'the', 'air', 'flow', 'therethrough', 'and', 'to', 'deposit', 'the', 'extracted', 'dust', 'and', 'other', 'dirt', 'in', 'a', 'chamber', '32', 'outside', 'the', 'cyclone', '22', 'and', 'separate', 'from', 'the', 'air', 'flow', 'through', 'the', 'casing', '10', 'of', 'the', 'appliance.', 'The', 'extracted', 'dirt', 'is', 'removed', 'from', 'the', 'ap', 'pliance', 'by', 'separation', 'of', 'the', 'cyclone', 'unit', '22', 'from', 'the', 'cas', 'ing', '10', '.', 'The', 'appliance', 'is', 'convertible', 'to', 'act', 'both', 'as', 'an', 'upright', 'type', 'cleaner', 'of', 'a', 'cylinder', 'type', 'cleaner.'], ['#', 'Vacuum', 'cleaners.', '##', 'Abstract', 'A', 'vacuum', 'cleaner', 'includes', 'a', 'dust', 'chamber', '1', ',', 'a', 'suction', 'chamber', '2', 'and', 'means', 'for', 'producing', 'low', 'pressure', 'in', 'the', 'sucking', 'chamber.', 'The', 'dust', 'chamber', '1', 'and', 'the', 'suction', 'chamber', '2', 'are', 'partitioned', 'from', 'one', 'another', 'by', 'a', 'first', 'filter', 'unit', '11', ',', 'and', 'the', 'suction', 'chamber', '2', 'is', 'provided', 'with', 'a', 'secondary', 'filter', 'unit', 'comprising', 'plural', 'filter', 'elements', '14', 'suspended', 'from', 'a', 'ceiling', '13', 'of', 'the', 'suction', 'chamber', '2', 'and', 'arranged', 'radially', 'thereof.', 'A', 'rotary', 'shaft', '16', 'provided', 'at', 'the', 'centre', 'of', 'the', 'radially', 'arranged', 'filter', 'elements', '14', 'supports', 'patting', 'vanes', '15', ',', 'each', 'of', 'which', 'is', 'designed', 'as', 'it', 'rotates', 'to', 'pat', 'each', 'filter', 'element', '14', 'on', 'an', 'inner', 'peripheral', 'portion', 'thereof,', 'thereby', 'causing', 'dust', 'entrained', 'therein', 'to', 'fall', 'off', 'by', 'vibration.', 'The', 'rotary', 'shaft', '16', 'is', 'additionally', 'provided', 'with', 'a', 'single', 'patting', 'arm', '17', 'extending', 'horizontally', 'so', 'as', 'to', 'pat', 'each', 'filter', 'element', '14', 'on', 'a', 'bottom', 'portion', 'thereof.', '##', 'Claims', 'CLAIMS', '1.', 'A', 'vacuum', 'cleaner', 'characterised', 'by', 'a', 'dust', 'chamber', '1', 'for', 'collecting', 'sucked', 'up', 'dust,', 'dirt', 'and', 'moisture', 'a', 'suction', 'chamber', '2', 'located', 'above', 'the', 'dust', 'chamber', '1', ',', 'the', 'suction', 'chamber', '2', 'including', 'an', 'air', 'inlet', '3', 'through', 'which,', 'in', 'use,', 'external', 'air', 'is', 'sucked', 'in', 'under', 'low', 'pressure', 'means', 'for', 'causing', 'a', 'low', 'suction', 'pressure', 'in', 'the', 'suction', 'chamber', '2', 'at', 'least', 'one', 'first', 'filter', 'element', '11', 'located', 'between', 'the', 'dust', 'chamber', '1', 'and', 'the', 'suction', 'chamber', '2', 'a', 'plurality', 'of', 'secondary', 'filter', 'elements', '14', 'suspended', 'from', 'a', 'ceiling', '13', 'of', 'the', 'suction', 'chamber', '2', ',', 'the', 'secondary', 'filter', 'elements', '14', 'being', 'radially', 'arranged', 'around', 'a', 'rotary', 'shaft', '16', 'supported', 'axially', 'of', 'the', 'suction', 'chamber', '2', 'a', 'patting', 'means', 'for', 'causing', 'dust', 'and', 'dirt', 'entrained', 'in', 'the', 'secondary', 'filter', 'elements', '14', 'to', 'fall', 'off', 'by', 'vibration,', 'the', 'patting', 'means', 'comprising', 'a', 'plurality', 'of', 'vanes', '15', 'fixed', 'to', 'the', 'rotary', 'shaft', '10', 'and', 'a', 'single', 'patting', 'arm', '13', 'extending', 'horizontally', 'therefrom', 'and', 'a', 'driving', 'means', 'for', 'rotating', 'the', 'rotary', 'shaft', '16', 'alternately', 'in', 'clockwise', 'and', 'anti', 'clockwise', 'directions.', '2.'], ['#', 'Compact', 'vacuum', 'cleaner.', '##', 'Abstract', 'A', 'vacuum', 'cleaner', '10', 'is', 'constructed', 'of', 'relatively', 'rigid,', 'molded', 'plastic', 'main', 'structural', 'elements', 'including', 'a', 'dirt', 'col', 'lecting', 'tank', '12', 'and', 'a', 'motor', 'housing', '14', 'releaseably', 'secured', 'to', 'each', 'other', 'by', 'a', 'buckle', '35.', 'The', 'main', 'structural', 'plastic', 'ele', 'ments', 'also', 'include', 'a', 'fan', 'housing', '30', 'wherein', 'a', 'centrifugal', 'fan', '51', 'rotates,', 'a', 'mounting', 'plate', '55', 'to', 'which', 'the', 'motor', '25', 'is', 'secured,', 'and', 'a', 'baffle', 'member', '60', 'having', 'a', 'cup', 'formation', 'which', 'receives', 'the', 'rear', 'of', 'the', 'motor', '25.', 'A', 'common', 'fasten', 'ing', 'means', '121', 'mechanically', 'secures', 'the', 'fan', 'housing', '30,', 'the', 'plate', '55,', 'and', 'the', 'baffle', 'member', '60', 'to', 'the', 'motor', 'housing', '14.', 'A', 'releaseable', 'latch', '34', 'is', 'at', 'one', 'end', 'of', 'the', 'buckle', '35', 'and', 'a', 'hook', '33', 'at', 'the', 'other', 'end', 'thereof.', 'The', 'latch', '34', 'holds', 'the', 'tank', '12', 'and', 'motor', 'housing', '14', 'together,', 'and', 'the', 'hook', '33', 'is', 'for', 'en', 'gaging', 'a', 'wall', 'bracket', '110', 'to', 'mount', 'the', 'cleaner', '10', 'in', 'a', 'ver', 'tical', 'position.', 'The', 'buckle', '35', 'also', 'includes', 'a', 'skid', 'portion', '32,', 'located', 'between', 'the', 'latch', '34', 'and', 'hook', '33,', 'to', 'facilitate', 'move', 'ment', 'of', 'the', 'cleaner', '10', 'along', 'a', 'horizontal', 'supporting', 'surface', '31.', 'Hundreds', 'of', 'relatively', 'small', 'apertures', '95', 'in', 'the', 'mount', 'ing', 'plate', '55', 'are', 'arranged', 'in', 'a', 'narrow', 'band', 'adja', 'cent', 'the', 'periphery', 'of', 'the', 'fan', '51', 'so', 'as', 'to', 'provide', 'for', 'the', 'pas', 'sage', 'of', 'air', 'through', 'the', 'motor', 'mounting', 'plate', '55', 'without', 'creating', 'excessive', 'back', 'pressure', 'while', 'maintaining', 'quiet', 'conditions', 'in', 'spite', 'of', 'high', 'speed', 'air', 'flow.']]\n"
     ]
    }
   ],
   "source": [
    "references = []\n",
    "\n",
    "for node in result.source_nodes:\n",
    "    # print(node.node.get_text())\n",
    "    node.node.get_text().split()\n",
    "    reference = (node.node.get_text().split())[2:]\n",
    "    references.append(reference)\n",
    "    print(reference)\n",
    "    print(\"======\")\n",
    "\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10785952054517312\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu(references, candidate)\n",
    "print(BLEUscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: Combination locks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: Liquid detergents for cleaning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "c:\\Users\\ryner\\Music\\sutd-t7-ir-rag\\ragfinal\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: Fertilisers for plants\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: Batteries for mobile devices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: Vacuum machines for home use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: Music players\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: Pulley systems for transportation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: Soil tilling machine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: Fire extinguishers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: Lubricants for joints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09744206418556337\n"
     ]
    }
   ],
   "source": [
    "def read_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "\n",
    "df = read_data(\"./eval_data.csv\")\n",
    "df.head()\n",
    "total_bleu = 0\n",
    "for query in df[\"query\"]:\n",
    "    result = await w.run(query=query, index=index)\n",
    "    candidate = str(result).split()\n",
    "    references = []\n",
    "\n",
    "    for node in result.source_nodes:\n",
    "        reference = (node.node.get_text().split())[2:]\n",
    "        references.append(reference)\n",
    "\n",
    "    BLEUscore = nltk.translate.bleu_score.sentence_bleu(references, candidate)\n",
    "    total_bleu += BLEUscore\n",
    "\n",
    "average_bleu = total_bleu / 10\n",
    "print(average_bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation Evaluation - ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: Combination locks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: Liquid detergents for cleaning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: Fertilisers for plants\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: Batteries for mobile devices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: Vacuum machines for home use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: Music players\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: Pulley systems for transportation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: Soil tilling machine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: Fire extinguishers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: Lubricants for joints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Scores across 10 queries: {'rouge1': 0.24726467661881912, 'rouge2': 0.048865808003594226, 'rougeL': 0.13369745521615128}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def calculate_rouge(candidate, references):\n",
    "    \"\"\"\n",
    "    Calculate ROUGE metrics (ROUGE-1, ROUGE-2, ROUGE-L) for a single candidate and its references.\n",
    "    \n",
    "    Args:\n",
    "        candidate (str): The retrieved/generated text by the model.\n",
    "        references (list of str): List of correct reference texts.\n",
    "    \n",
    "    Returns:\n",
    "        dict: ROUGE-1, ROUGE-2, and ROUGE-L average scores for the candidate against all references.\n",
    "    \"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "    for ref in references:\n",
    "        score = scorer.score(candidate, ref)\n",
    "        scores[\"rouge1\"].append(score[\"rouge1\"].fmeasure)\n",
    "        scores[\"rouge2\"].append(score[\"rouge2\"].fmeasure)\n",
    "        scores[\"rougeL\"].append(score[\"rougeL\"].fmeasure)\n",
    "    avg_scores = {metric: sum(values) / len(values) if values else 0.0 for metric, values in scores.items()}\n",
    "    return avg_scores\n",
    "\n",
    "# Run through multiple queries\n",
    "queries = [\"Vacuum cleaners\", \"Smartphones\", \"Electric cars\", \"Solar panels\", \"AI models\",\n",
    "           \"Machine learning\", \"Cooking recipes\", \"History books\", \"Space exploration\", \"Quantum computing\"]\n",
    "all_rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "\n",
    "for query in df[\"query\"]:\n",
    "    result = await w.run(query=query, index=index)\n",
    "    candidate = \" \".join(str(result).split())  # Ensure candidate is a single string\n",
    "    references = []\n",
    "\n",
    "    for node in result.source_nodes:\n",
    "        reference = \" \".join(node.node.get_text().split()[2:])  # Process and tokenize reference\n",
    "        references.append(reference)\n",
    "\n",
    "    if references:\n",
    "        rouge_scores = calculate_rouge(candidate, references)\n",
    "        for metric in all_rouge_scores:\n",
    "            all_rouge_scores[metric].append(rouge_scores[metric])\n",
    "    else:\n",
    "        print(f\"No references found for query: {query}\")\n",
    "\n",
    "# Calculate average ROUGE scores across all queries\n",
    "avg_rouge_scores = {metric: sum(values) / len(values) if values else 0.0 for metric, values in all_rouge_scores.items()}\n",
    "\n",
    "# Print final results\n",
    "print(f\"Average ROUGE Scores across {len(queries)} queries: {avg_rouge_scores}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragfinal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
